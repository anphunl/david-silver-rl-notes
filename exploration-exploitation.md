# Reinforcement Learning: Exploration and Exploitation (Lecture 9)

A very fundamental tradeoff in reinforcement learning is that of exploration
versus exploitation. Do we pick the action that we currently know is best, or do
we pick a new action that we know nothing of, in the hope that it will give us a
better reward? For the latter case, we have to consider that we are potentially
giving up reward in the short term, for long term benefit (or not!).

A basic distinction within this domain is that of state-action space exploration
vs. parameter exploitation. The former is more what we think of and what I
described above. The latter deals with exploration of parameters, for example in
the case of policy gradient methods, where the policy is parameterized by
trainable parameters $\theta$. We are mostly interested in the former idea,
however.

## Multi-Armed Bandit

The *multi-armed bandit* problem is a very simple model that we can investigate
to better understand the exploration/exploitation tradeoff. In this MDP, we have
no state, but only actions and a reward function, i.e. $(\mathcal{A, R})$. Here,
$\mathcal{A}$ is a finite set of actions ("bandit arms"), while $\mathcal{R}$ is
a distribution over rewards for actions: $\mathcal{R}^a(r) = \Pr[R = r | A =
a]$. At each step, then, the agent selects an action $A_t \in \mathcal{A}$ and
the environment generates a reward ("payout") $R_t \sim \mathcal{R}^{A_t}$. The
goal, as always, is to maximize the cumulative reward $\sum_{\tau=1}^t
R_{\tau}$.

We can define a few more functions and variables in this setup. The
*action-value* (Q-value) of an action is the mean reward for that action:

$$q(a) = \mathbb{E}[R | A = a].$$

Furthermore, there exists __one__ optimal value $v_{\star}$, which is the
Q-value of the best action $a^{\star}$:

$$v_\star = q(a^\star) = \max_{a \in \mathcal{A}} q(a).$$

Furthermore, we'll now want to investigate a novel quantity known as the
*regret*. Regret gives us an indication of the opportunity loss for picking a
particular action $A_t$ compared to the optimal action $a^\star$. In symbols:

$$I_t = \mathbb{E}[v_\star - q(A_t)].$$

Finally, we can define the *total regret* as the total opportunity loss over an
entire episode:

$$L_t = \mathbb{E}\left[\sum_{\tau=1}^t v_\star - q(A_\tau)\right].$$

The goal of the agent is then to minimize the total regret which is equivalent
to maximizing the cumulative (total) reward. In addition to the above
definitions, we'll now also keep a count $N_t(a)$ which is the *expected number
of selections for an action $a$*. Moreover, we define the *gap* $\delta_a$ as
the difference between the *expected* value $q(a)$ of an action $a$ and the
optimal action:

$$\Delta_a = v_\star - q(a).$$

As such, we can give an equivalent definition of the regret $L_t$ in terms of
the gap:

$$
\begin{align}
  L_t &= \mathbb{E}\left[\sum_{\tau=1}^t v_\star - q(A_\tau)\right] \\
      &= \sum_{a \in \mathcal{A}}\mathbb{E}[N_t(a)](v_\star - q(a)) \\
      &= \sum_{a \in \mathcal{A}}\mathbb{E}[N_t(a)]\Delta_a.
\end{align}
$$

Now, if we think of the regret as a function of iterations, we can make some
observations. For example, we observe that the regret of a greedy algorithm $A_t =
\argmax_{a \in \mathcal{A}} Q_t(a)$ is a linear function, i.e. it increases
linearly with each iteration. The reason why is that we may "lock" onto a
suboptimal action forever, thus adding a certain fixed amount of regret each
time. An alteration that we can make here is to initialize the Q-value of each
action to the maximum reward. This is called *optimistic initialization*. Note
that updates to $Q(a)$ are made via an averaging process:

$$Q(a) = \frac{1}{N_t(a)}\sum_{t=1}^T \mathbf{1}(A_t = a)R_t.$$

Now, while $\varepsilon$-greedy approaches incur linear regret, certain
strategies that decay $\varepsilon$ can actually only incur logarithmic
(asymptotic) regret. Either way, there is actually a lower bound on the regret
that any algorithm can achieve (i.e. no algorithm can do better), which is
logarithmic:

$$
\lim_{t\rightarrow\infty} L_t \geq \log t \sum_{a \,|\, \Delta_a > 0} \frac{\Delta_A}{\text{KL}(\mathcal{R}^a || \mathcal{R}^{a^\star})}.
$$

As you can see, this lower bound is proportional to the gap size (higher gap
size means higher lower-bound for regret, i.e. more regret) and indirectly
proportional to the similarity of bandits, given by the Kullback-Leibler
divergence.

### UCB

A very fundamental idea within the exploration/exploitation domain is that of
*optimism in the face of uncertainty*. This idea tells us that if we know very
well of the value of one action, but not as well about the value of another
action, but do know that that other value *may* have a greater value, then we
should go for the other action. So if you imagine a Gaussian distribution for
the first action and a Gaussian for the second, then if the second Gaussian has
a higher tail such that it *could* have its mean higher than the first Gaussian,
even if the first Gaussian has currently a greater mean (but shorter tail), then
we should pick the second one.

To formalize this idea, we can think of *confidence bounds*. Let $U_t(a)$ be an
upper confidence bound on the value of action $a$ at time $t$, such that with
high probability, the expected value $q(a)$ is bounded by the current value
$Q_t(a)$ plus this bound:

$$q(a) \geq Q_t(a) + U_t(a).$$

Then what the above paragraph described is equivalent to saying that we should
pick the action with the highest value for $Q_t(a) + U_t(a)$, i.e.

$$A_t = \argmax_{a \in \mathcal{A}} Q_t(a) + U_t(a).$$

Furthermore, one property of these upper confidence bounds that we require is
that they should get smaller over time, meaning the variance should become
smaller and smaller (the Gaussian becomes thinner).

So how do we find $U_t(a)$? To do this, we'll use *Hoeffding's inequality*,
which says that for some independently and identically distributed (i.i.d)
random variables $X_1, ..., X_t \in [0, 1]$ with sample mean $\bar{X} =
\frac{1}{t}\sum_\tau^t X_{\tau}$, we have

$$\Pr[\mathbb{E}[X] > \bar{X} + u] \leq e^{-2tu^2}$$

as an upper bound on the probability that the true mean of $X$ will be greater
than the sample mean $\bar{X}$ plus some value $u$. Differently put, this is an
upper bound on the probability that the difference between the true mean and the
sample mean will be greater than some value $u$. For our purposes, we an plug in
$q(a)$ for the expectation (true mean), $Q_t(a)$ for the sample mean and our
upper confidence bound $U_t(a)$ for the bounding value. This gives us

$$\Pr[q(a) > Q_t(a) + U_t(a)] \leq e^{-2N_t(a)U_t(a)^2}.$$

We can now use this inequality to solve for $U_t(a)$, giving us a way to compute
this quantity. If we set $p$ to be some probability that we want for  our
confidence interval, we get

$$
\begin{align}
  e^{-2N_t(a)U_t(a)^2} &= p \\
  -2N_t(a)U_t(a)^2 &= \log p \\
  U_t(a)^2 &= \frac{\log p}{-2N_t(a)} \\
  U_t(a) &= \sqrt{\frac{\log p}{-2N_t(a)}}.
\end{align}
$$

As we can see, this gives us precisely the property that we wanted: Since
$N_t(a)$ is in the denominator, this upper bound will decrease over time, giving
us more and more certainty about the true mean of the action-value $Q(a)$.

Now that we have a way to compute $U_t(a)$, we know how to pick actions
according to the formula we defined earlier. We can now develop an algorithm to
solve multi-armed bandit problems, called the *UCB1* algorithm. It picks an
action according

$$
\begin{align}
  A_t &= \argmax_{a \in \mathcal{A}} Q_t(a) + U_t(a) \\
      &= \argmax_{a \in \mathcal{A}} Q_t(a) + \sqrt{\frac{\log p}{-2N_t(a)}}
\end{align}
$$

This algorithm achieves the logarithmic regret we discussed earlier. Another
algorithm which achieves this bound is called *Thompson sampling*, which sets
the policy to

$$\pi(a) = \mathbb{E}[\mathbf{1}(Q(a) = \max_{a'}Q(a')) | R_1,...,R_{t-1}]$$

where use *Bayes law* to compute a posterior distribution $p_{\mathbf{w}}(Q |
R_1,...,R_{t-1})$ and then sample an action-value function $Q(a)$ from the
posterior. We then simply pick the action that maximizes the action value
functions.
